import os
import time
import random
import hashlib
import argparse
from colorama import init, Fore, Style
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException,
    StaleElementReferenceException,
    InvalidSelectorException
)
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header
from email.utils import formataddr
import pandas as pd
from datetime import datetime

init(autoreset=True)

# ====================== æ ¸å¿ƒé…ç½® ======================
MAX_PAGES = 999  # æœ€å¤§çˆ¬å–é¡µæ•°
SCROLL_PAUSE = 1  # æ»šåŠ¨ç­‰å¾…æ—¶é—´
RETRY_LIMIT = 10  # å…ƒç´ é‡è¯•æ¬¡æ•°
VERIFICATION_TIME = 1  # å»¶é•¿éªŒè¯ç å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
CONSECUTIVE_SAME_LIMIT = 5  # é™ä½è¿ç»­ç›¸åŒé¡µé¢é˜ˆå€¼ï¼Œé¿å…æ— æ•ˆå¾ªç¯
CONTENT_TIMEOUT = 30  # å»¶é•¿å†…å®¹åŠ è½½è¶…æ—¶æ—¶é—´
# æ–‡æ¡£ç±»å‹æ‰©å±•å
DOC_EXTENSIONS = ('.pdf', '.docx', '.doc', '.rar', '.inc', '.txt', '.sql',
                  '.conf', '.xlsx', '.xls', '.csv', '.ppt', '.pptx')
# éœ€è¿‡æ»¤çš„æ‰©å±•å
FILTER_EXTENSIONS = ('.apk',)


# ====================== æ–°å¢é…ç½® ======================

def print_banner():
    """æ‰“å°ç¨‹åºçš„æ¨ªå¹…ä¿¡æ¯"""
    banner = r"""
                    /|\ 
                  /  |  \
                '/ ` |   '\ 
                |    |    |
                |    |    |
                |    |    |
                |    |    |
                |    |    |
                |    |    |
                |    |    |
                |    |    |
                |    |    |
                |'.Â·Â´|`Â·.'|
                |;::;|;::;|
                |;::;|;::;|
                |;::;|;::;|
                |;::;|;::;|
                |.Â·Â´Â¯`Â·.| 
                |;      ';|
                |`Â·._.Â·Â´|
                |;::;|;::;|
 |Â¯`Â·._.Â·Â´Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯`Â·._.Â·Â´Â¯|
 |              Bifish                '| 
 |.Â·Â´Â¯`Â·.__________ '.Â·Â´Â¯`Â·.| 
 |.Â·Â´Â¯`Â·.___         '___.Â·Â´Â¯`Â·.| 
               .Â·Â´      `Â·. 
               |.Â·Â´Â¯`Â·..Â·Â´| 
               |`Â·._.Â·Â´`Â·.| 
               |.Â·Â´Â¯`Â·..Â·|
               |`Â·._.Â·Â´`Â·.| 
               |.Â·Â´Â¯`Â·..Â·|
               `Â·.      .Â·Â´ 
            . Â· Â´Â¯Â¯Â¯Â¯` Â· . 
          ,' ,'  .Â·Â´Â¯`Â·.   ', ' 
          ', ',  `Â·._.Â·Â´  ,' , ' 
            '  ,_____,  ' 
                 `Â·..Â·Â´ 
    """
    print(Fore.CYAN + banner)
    print(Fore.GREEN + "=" * 60)
    print(Fore.GREEN + "  EdgeURL åŸºäºEdgeæµè§ˆå™¨çš„URLçˆ¬å–å™¨ï¼ˆv2.3ç‰ˆï¼‰ - è¾‰å°é±¼")
    print(Fore.GREEN + "=" * 60)
    print(Style.RESET_ALL)


def setup_driver(proxy=None):
    """è®¾ç½®å¹¶è¿”å› Edge æµè§ˆå™¨é©±åŠ¨ï¼Œä¿®å¤æ½œåœ¨çš„SSLå’ŒSmartScreené—®é¢˜"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        driver_path = os.path.join(current_dir, "msedgedriver.exe")

        if not os.path.exists(driver_path):
            print(Fore.RED + f"[-] æœªæ‰¾åˆ° Edge é©±åŠ¨: {driver_path}")
            print(Fore.YELLOW + "[!] è¯·ç¡®ä¿ msedgedriver.exe åœ¨è„šæœ¬åŒä¸€ç›®å½•ä¸‹")
            return None

        try:
            from selenium.webdriver.edge.options import Options
            options = Options()
        except ImportError:
            options = webdriver.EdgeOptions()

        # æ–°å¢ï¼šç¦ç”¨SmartScreenä»¥è§£å†³ç›¸å…³é”™è¯¯
        options.add_argument("--disable-features=EdgeSmartScreen")
        # æ–°å¢ï¼šå¿½ç•¥SSLé”™è¯¯
        options.add_argument("--ignore-certificate-errors")
        options.add_argument("--ignore-ssl-errors")

        options.add_argument("--start-maximized")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
        )

        if proxy:
            options.add_argument(f'--proxy-server={proxy}')

        service = webdriver.edge.service.Service(driver_path)
        driver = webdriver.Edge(service=service, options=options)

        # éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                window.chrome = {runtime: {}}; 
                Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh']});
            """
        })
        return driver
    except Exception as e:
        print(Fore.RED + f"[-] é©±åŠ¨è®¾ç½®å¤±è´¥: {e}")
        return None


def auto_scroll(driver):
    """è‡ªåŠ¨æ»šåŠ¨é¡µé¢ä»¥ç¡®ä¿å†…å®¹å®Œå…¨åŠ è½½"""
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(SCROLL_PAUSE)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
        time.sleep(1)
    except Exception as e:
        print(Fore.RED + f"[-] è‡ªåŠ¨æ»šåŠ¨å¤±è´¥: {e}")


def get_page_content_hash(driver):
    """è·å–é¡µé¢ä¸»ä½“å†…å®¹çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºåˆ¤æ–­å†…å®¹æ˜¯å¦å˜åŒ–ï¼‰"""
    try:
        content_elements = driver.find_elements(By.CSS_SELECTOR, 'li.b_algo')
        content_text = " ".join([el.text for el in content_elements])
        return hashlib.md5(content_text.encode('utf-8')).hexdigest()
    except Exception as e:
        print(Fore.RED + f"[-] è·å–é¡µé¢å†…å®¹å“ˆå¸Œå¤±è´¥: {e}")
        return hashlib.md5(driver.page_source.encode('utf-8')).hexdigest()


def find_next_page(driver):
    """æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®ï¼ˆå…¼å®¹å¤šç§å½¢æ€ï¼‰"""
    selectors = [
        'a[title="Next page"]',
        'a[aria-label="Next page"]',
        'a.sb_pagN',
    ]
    xpath_selectors = [
        '//a[contains(text(), "ä¸‹ä¸€é¡µ")]',
        '//a[contains(text(), "Next")]',
    ]

    for selector in selectors:
        try:
            btns = driver.find_elements(By.CSS_SELECTOR, selector)
            if btns:
                return btns[0]
        except InvalidSelectorException:
            print(Fore.RED + f"[-] æ— æ•ˆçš„ CSS é€‰æ‹©å™¨: {selector}")
            continue

    for xpath in xpath_selectors:
        try:
            btns = driver.find_elements(By.XPATH, xpath)
            if btns:
                return btns[0]
        except Exception as e:
            print(Fore.RED + f"[-] æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®å¤±è´¥: {e}")
            continue

    return None


def is_valid_url(url, base_domain):
    """åˆ¤æ–­ URL æ˜¯å¦æœ‰æ•ˆï¼Œæ–°å¢è¿‡æ»¤é€»è¾‘"""
    # è¿‡æ»¤.apkç»“å°¾çš„URL
    if url.endswith(FILTER_EXTENSIONS):
        return False

    # è¿‡æ»¤è·¯å¾„ä¸­åŒ…å«/jpgçš„URL
    if '/jpg' in url:
        return False

    excluded_domains = [
        'cn.bing.com', 'microsoft.com', 'bing.com', 'baidu.com',
        'google.com', 'wikipedia.org', 'github.com', 'stackoverflow.com'
    ]

    if base_domain not in url:
        return False

    for domain in excluded_domains:
        if domain in url:
            return False

    return True


def classify_urls(url_list):
    """åˆ†ç±»URLä¸ºæ™®é€šURLå’Œæ–‡æ¡£URL"""
    normal_urls = []
    doc_urls = []

    for url in url_list:
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æ¡£ç±»å‹
        if url.lower().endswith(DOC_EXTENSIONS):
            doc_urls.append(url)
        else:
            normal_urls.append(url)

    return normal_urls, doc_urls


def extract_bing_urls(driver, base_domain):
    """æå–Bingæœç´¢ç»“æœçš„URLå’Œæ ‡é¢˜ï¼Œå¹¶è¿›è¡Œåˆæ­¥è¿‡æ»¤"""
    urls = []  # å­˜å‚¨æ ¼å¼ï¼š[(url, title), ...]
    try:
        # å®šä½æ‰€æœ‰æœç´¢ç»“æœé¡¹
        result_items = WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#b_results > li.b_algo'))
        )

        for item in result_items:
            try:
                # æå–URL
                link_element = item.find_element(By.CSS_SELECTOR, 'h2 a')
                link = link_element.get_attribute('href')

                # æå–æ ‡é¢˜
                title = link_element.text.strip()

                if link and is_valid_url(link, base_domain):
                    urls.append((link, title))
            except Exception as e:
                print(Fore.RED + f"[-] æå–å•ä¸ªURL/æ ‡é¢˜å¤±è´¥: {e}")
                continue
    except Exception as e:
        print(Fore.RED + f"[-] æå–æœç´¢ç»“æœå¤±è´¥: {e}")
    return urls


def save_to_excel(url_list, base_domain, is_document=False):
    """ä¿å­˜URLå’Œæ ‡é¢˜åˆ°Excelï¼Œæ”¯æŒæ™®é€šURLå’Œæ–‡æ¡£URLçš„ä¸åŒè·¯å¾„"""
    try:
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(url_list, columns=['URL', 'æ ‡é¢˜'])

        # æ„å»ºä¿å­˜è·¯å¾„
        if is_document:
            result_dir = os.path.join('results', base_domain, 'çˆ¬å–çš„æ–‡æ¡£')
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            excel_file = os.path.join(result_dir, f'{base_domain}_æ–‡æ¡£_{timestamp}.xlsx')
        else:
            result_dir = os.path.join('results', base_domain)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            excel_file = os.path.join(result_dir, f'Edge_results_{base_domain}_{timestamp}.xlsx')

        os.makedirs(result_dir, exist_ok=True)
        df.to_excel(excel_file, index=False)
        print(Fore.GREEN + f"[+] Excel æ–‡ä»¶å·²ä¿å­˜: {excel_file}")
        return excel_file
    except Exception as e:
        print(Fore.RED + f"[-] ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
        return None


def crawl_domain(driver, query, proxy=None):
    """çˆ¬å–å•ä¸ªåŸŸåç›¸å…³çš„ URL å’Œæ ‡é¢˜"""
    base_domain = query.split(':')[1]
    all_normal_urls = []  # æ™®é€šURL [(url, title), ...]
    all_doc_urls = []  # æ–‡æ¡£URL [(url, title), ...]
    page_num = 1
    consecutive_same_count = 0

    print(Fore.YELLOW + f"[+] æ­£åœ¨çˆ¬å– {base_domain} çš„ç›¸å…³ URL...")
    driver.get(f"https://www.bing.com/search?q={query}")
    print(Fore.YELLOW + f"[+] æ‰‹åŠ¨å¤„ç†éªŒè¯ç ï¼ˆè‹¥æœ‰ï¼‰ï¼Œ{VERIFICATION_TIME} ç§’åç»§ç»­...")
    time.sleep(VERIFICATION_TIME)

    prev_url = driver.current_url
    prev_content_hash = get_page_content_hash(driver)

    while page_num <= MAX_PAGES:
        print(Fore.YELLOW + f"\n[+] ç¬¬ {page_num} é¡µ | å¼€å§‹çˆ¬å–")
        auto_scroll(driver)

        page_urls = extract_bing_urls(driver, base_domain)
        # åˆ†ç±»URLå’Œæ ‡é¢˜
        normal_urls = []
        doc_urls = []
        for url, title in page_urls:
            if url.lower().endswith(DOC_EXTENSIONS):
                doc_urls.append((url, title))
            else:
                normal_urls.append((url, title))

        all_normal_urls.extend(normal_urls)
        all_doc_urls.extend(doc_urls)

        # æœ‰æœ‰æ•ˆURLæ‰æ‰“å°ç»Ÿè®¡
        has_new_urls = len(normal_urls) + len(doc_urls) > 0
        if has_new_urls:
            print(
                Fore.GREEN + f"[+] ç¬¬ {page_num} é¡µ | æ–°å¢æ™®é€š URL: {len(normal_urls)} ä¸ª | æ–°å¢æ–‡æ¡£ URL: {len(doc_urls)} ä¸ª")
            # æ™®é€šURLï¼ˆè“è‰²ï¼‰
            if normal_urls:
                print(Fore.CYAN + "[+] æ–°å¢æ™®é€š URL åˆ—è¡¨ï¼š")
                for idx, (url, title) in enumerate(normal_urls, 1):
                    print(Fore.CYAN + f"    {idx}. {url}")
                    print(Fore.WHITE + f"       æ ‡é¢˜: {title}")
            # æ–‡æ¡£URLï¼ˆç´«è‰²ï¼‰
            if doc_urls:
                print(Fore.MAGENTA + "[+] æ–°å¢æ–‡æ¡£ URL åˆ—è¡¨ï¼š")
                for idx, (url, title) in enumerate(doc_urls, 1):
                    print(Fore.MAGENTA + f"    {idx}. {url}")
                    print(Fore.WHITE + f"       æ ‡é¢˜: {title}")

        # æŸ¥æ‰¾ä¸‹ä¸€é¡µï¼ˆåŸé€»è¾‘ä¸å˜ï¼‰
        next_btn = find_next_page(driver)
        if not next_btn:
            print(Fore.RED + "[-] æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œç»ˆæ­¢çˆ¬å–")
            break

        try:
            current_url = driver.current_url
            current_content_hash = get_page_content_hash(driver)
            driver.execute_script("arguments[0].scrollIntoView(true);", next_btn)
            next_btn.click()

            WebDriverWait(driver, CONTENT_TIMEOUT).until(
                lambda d: d.current_url != current_url or
                          get_page_content_hash(d) != current_content_hash
            )

            new_url = driver.current_url
            new_content_hash = get_page_content_hash(driver)

            if new_url == current_url and new_content_hash == current_content_hash:
                consecutive_same_count += 1
                print(Fore.RED + f"[!] é¡µé¢æœªåˆ·æ–°ï¼è¿ç»­ {consecutive_same_count} æ¬¡")
            else:
                consecutive_same_count = 0

            prev_url = new_url
            prev_content_hash = new_content_hash

            if consecutive_same_count >= CONSECUTIVE_SAME_LIMIT:
                print(Fore.RED + f"[!] è¿ç»­ {CONSECUTIVE_SAME_LIMIT} æ¬¡é¡µé¢æœªåˆ·æ–°ï¼Œç»ˆæ­¢çˆ¬å–")
                break

            page_num += 1
            time.sleep(random.uniform(2, 5))

        except TimeoutException:
            print(Fore.RED + "[-] é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»ˆæ­¢çˆ¬å–")
            break
        except Exception as e:
            print(Fore.RED + f"[-] ç¿»é¡µè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            break

    return all_normal_urls, all_doc_urls, page_num - 1


def generate_email_content(domain_stats, total_domains, total_urls, total_pages, execution_time):
    """ç”Ÿæˆè¯¦ç»†çš„é‚®ä»¶å†…å®¹"""
    content = f"""
ğŸ“Š EdgeURL çˆ¬å–ä»»åŠ¡å®ŒæˆæŠ¥å‘Š ğŸ“Š
å°Šæ•¬çš„è¾‰å°é±¼å…ˆç”Ÿï¼š
EdgeURL çˆ¬è™«å·²å®Œæˆæ‰€æœ‰åŸŸåçš„çˆ¬å–å·¥ä½œï¼ä»¥ä¸‹æ˜¯è¯¦ç»†æŠ¥å‘Šï¼š
ğŸ“‹ æ€»ä½“ç»Ÿè®¡ï¼š
  â€¢ å¤„ç†åŸŸåæ€»æ•°ï¼š{total_domains} ä¸ª
  â€¢ æ€»çˆ¬å–é¡µæ•°ï¼š{total_pages} é¡µ
  â€¢ æ€»è·å– URLï¼š{total_urls} ä¸ª
  â€¢ æ‰§è¡Œæ—¶é—´ï¼š{execution_time:.2f} ç§’
ğŸ” å„åŸŸåè¯¦ç»†ç»“æœï¼š
"""
    for domain, stat in domain_stats.items():
        content += f"""
  â€¢ {domain}:
    - çˆ¬å–é¡µæ•°ï¼š{stat.get('pages', 0)}
    - è·å–æ™®é€š URL æ•°é‡ï¼š{stat.get('normal_urls', 0)}
    - è·å–æ–‡æ¡£ URL æ•°é‡ï¼š{stat.get('doc_urls', 0)}
    - æ™®é€šç»“æœä¿å­˜è·¯å¾„ï¼šresults/{domain}/
    - æ–‡æ¡£ç»“æœä¿å­˜è·¯å¾„ï¼šresults/{domain}/çˆ¬å–çš„æ–‡æ¡£/
"""
    content += """
ğŸ’¡ è¯´æ˜ï¼š
- æ–‡æ¡£ç±»å‹URLå·²å•ç‹¬ä¿å­˜
- ç”±äºBingçš„åçˆ¬æœºåˆ¶ï¼Œéƒ¨åˆ†URLå¯èƒ½æ— æ³•è·å–
ğŸš€ ç¥æ‚¨æ¸—é€æµ‹è¯•é¡ºåˆ©ï¼Œå¿…å‡ºé«˜å±æ¼æ´ï¼
EdgeURL çˆ¬è™«åŠ©æ‰‹ ğŸ¤–
"""
    return content


def send_email(sender_email, sender_password, receiver_email, subject, content):
    """å‘é€é‚®ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼Œä¼˜åŒ–é”™è¯¯æ•è·ä¸åé¦ˆï¼‰"""
    try:
        print(Fore.YELLOW + "[+] å‡†å¤‡å‘é€é‚®ä»¶...")
        message = MIMEMultipart()
        message['From'] = formataddr((str(Header("EdgeURL çˆ¬è™«åŠ©æ‰‹", 'utf-8')), sender_email))
        message['To'] = receiver_email
        message['Subject'] = Header(subject, 'utf-8')
        message.attach(MIMEText(content, 'plain', 'utf-8'))

        print(Fore.YELLOW + "[+] è¿æ¥ SMTP æœåŠ¡å™¨...")
        with smtplib.SMTP_SSL('smtp.qq.com', 465) as server:
            server.set_debuglevel(0)
            print(Fore.YELLOW + "[+] æ­£åœ¨è¿›è¡Œèº«ä»½éªŒè¯...")
            server.login(sender_email, sender_password)
            print(Fore.YELLOW + "[+] èº«ä»½éªŒè¯æˆåŠŸï¼Œæ­£åœ¨å‘é€é‚®ä»¶...")
            server.sendmail(sender_email, [receiver_email], message.as_string())
            print(Fore.GREEN + "[âˆš] é‚®ä»¶å‘é€æˆåŠŸï¼")
            return True
    except smtplib.SMTPException as e:
        if "b'\\x00\\x00\\x00'" in str(e):
            print(Fore.YELLOW + "[!] å‡ºç° SMTP é€šä¿¡ç‰¹æ®Šå¼‚å¸¸ï¼Œé‚®ä»¶å¯èƒ½å·²å‘é€æˆåŠŸ")
            return True
        else:
            print(Fore.RED + f"[-] SMTP é”™è¯¯: {str(e)}")
            print(Fore.YELLOW + "[!] è¯·æ£€æŸ¥é‚®ç®±è´¦å·ã€æˆæƒç å’Œç½‘ç»œè¿æ¥")
    except Exception as e:
        print(Fore.RED + f"[-] å‘é€é‚®ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
    return False


def main():
    print_banner()
    parser = argparse.ArgumentParser(description='Bing ç›¸å…³ URL çˆ¬å–ï¼ˆä¼˜åŒ–ç‰ˆï¼‰')
    parser.add_argument('-f', '--file', type=str, default='domain.txt', help='åŸŸååˆ—è¡¨æ–‡ä»¶ï¼Œé»˜è®¤ä¸º domain.txt')
    parser.add_argument('--proxy', type=str, default=None, help='ä»£ç†ï¼Œå¦‚ 127.0.0.1:7890')
    args = parser.parse_args()

    if not os.path.exists(args.file):
        print(Fore.RED + f"[-] æœªæ‰¾åˆ°æ–‡ä»¶: {args.file}")
        return

    with open(args.file, 'r', encoding='utf-8') as f:
        domains = [line.strip() for line in f.readlines() if line.strip()]

    if not domains:
        print(Fore.RED + f"[-] æ–‡ä»¶ {args.file} ä¸ºç©ºæˆ–å…¨éƒ¨ä¸ºç©ºè¡Œ")
        return

    driver = setup_driver(args.proxy)
    if not driver:
        return

    domain_stats = {}
    total_domains = len(domains)
    total_normal_urls = 0
    total_doc_urls = 0
    total_pages = 0
    start_time = time.time()

    try:
        for domain in domains:
            domain_start_time = time.time()
            query = f'site:{domain}'
            # çˆ¬å–å¹¶è·å–åˆ†ç±»åçš„URL
            normal_urls, doc_urls, pages = crawl_domain(driver, query, args.proxy)

            normal_count = len(normal_urls)
            doc_count = len(doc_urls)

            total_normal_urls += normal_count
            total_doc_urls += doc_count
            total_pages += pages

            domain_stats[domain] = {
                'pages': pages,
                'normal_urls': normal_count,
                'doc_urls': doc_count
            }

            # ä¿å­˜æ™®é€šURL
            normal_file = save_to_excel(normal_urls, domain, is_document=False)
            # ä¿å­˜æ–‡æ¡£URL
            doc_file = save_to_excel(doc_urls, domain, is_document=True) if doc_urls else None

            if normal_file or doc_file:
                print(Fore.GREEN + f"\n[+] çˆ¬å– {domain} å®Œæˆ | é¡µæ•°: {pages} | "
                                   f"æ™®é€šURL: {normal_count} | æ–‡æ¡£URL: {doc_count} | "
                                   f"è€—æ—¶: {time.time() - domain_start_time:.2f} ç§’")

            delay = random.uniform(3, 7)
            print(Fore.YELLOW + f"[+] å‡†å¤‡çˆ¬å–ä¸‹ä¸€ä¸ªåŸŸåï¼Œ{delay:.1f} ç§’åç»§ç»­...")
            time.sleep(delay)

    except Exception as e:
        print(Fore.RED + f"[-] çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        if driver:
            print(Fore.YELLOW + "\n[+] æ‰€æœ‰åŸŸåçˆ¬å–å®Œæˆï¼Œå…³é—­æµè§ˆå™¨...")
            driver.quit()

    execution_time = time.time() - start_time
    total_urls = total_normal_urls + total_doc_urls
    email_content = generate_email_content(domain_stats, total_domains, total_urls, total_pages, execution_time)

    config = {
        "sender_email": "1794686508@qq.com",
        "sender_password": "busnjcluyxtlejgc",
        "receiver_email": "shenghui3301@163.com",
        "subject": f"ğŸ“§ EdgeURL çˆ¬å–å®Œæˆï¼å…±è·å– {total_urls} ä¸ªURL",
        "content": email_content
    }

    email_sent = send_email(**config)
    if email_sent:
        print(Fore.GREEN + "\n[âœ“] çˆ¬å–å’Œé€šçŸ¥æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    else:
        print(Fore.RED + "\n[-] çˆ¬å–å®Œæˆï¼Œä½†é‚®ä»¶é€šçŸ¥å¤±è´¥")
        print(Fore.YELLOW + "[*] è¯·æ£€æŸ¥é‚®ç®±é…ç½®å’Œç½‘ç»œè¿æ¥")

    input(Fore.YELLOW + "\n[*] æŒ‰å›è½¦é€€å‡º...")


if __name__ == "__main__":
    main()